---
title: "Pluralsight Data Scientist Take-Home Exercise"
subtitle: "Response to Questions"
author: "Trenton Pulsipher"
date: "`r lubridate::today()`"
output: html_document
---

```{r setup, echo = F, warning = F, message = F, error = F}
knitr::opts_chunk$set(
  echo = F,
  message = F,
  warning = F,
  error = F,
  fig.height = 3,
  fig.width = 9.5,
  cache = F
)

# Libraries 
library(DBI)
library(RSQLite)
library(lubridate)
library(tidyverse)
library(trelliscopejs)
library(HSPSUtils) # install_github("HSPS-DataScience/HSPSUtils")
                   # devtools::update_packages("HSPSUtils")
library(rbokeh)
library(ggpubr)
library(matlab)
library(kableExtra)
```


```{r dataIn, echo = F, warning = F, message = F, error = F}
# connect the database created by running .sql file in TablePlus
con <- DBI::dbConnect(RSQLite::SQLite(), "~/Documents/Development/R/pluralsightTest/pluralsightTestDB.sqlite3")

# read in data from the four database tables
question_details <- DBI::dbReadTable(con, "question_details") %>% 
  as_tibble() %>%
  mutate(date_created = as.POSIXct(as.numeric(date_created), origin = '1970-01-01', tz = "GMT"))

question_interactions <- dbReadTable(con, "question_interactions") %>% 
  as_tibble() %>%
  mutate(date_created = ymd_hms(date_created))

user_assessment_sessions <- dbReadTable(con, "user_assessment_sessions") %>% 
  as_tibble() %>%
  mutate(
    date_created = ymd_hms(date_created),
    date_modified = ymd_hms(date_modified)
  )

user_interactions <- dbReadTable(con, "user_interactions") %>% 
  as_tibble() %>%
  mutate(date_created = ymd_hms(date_created))

## Join the data together
d <- user_assessment_sessions %>%
  left_join(user_interactions, by = "user_assessment_session_id") %>%
  rename(
    ranking_overall = ranking.x,
    rd_overall = rd.x,
    display_score_overall = display_score.x,
    percentile_overall = percentile.x,
    date_created_overall = date_created.x,
    ranking_interaction = ranking.y,
    rd_interaction = rd.y,
    display_score_interaction = display_score.y,
    percentile_interaction = percentile.y,
    date_created_interation = date_created.y
  ) %>%
  left_join(question_interactions %>%
              rename(
                result_questions = result,
                ranking_questions = ranking,
                rd_questions = rd
              ) %>%
              left_join(question_details, by = "item_content_id") %>%
              rename(
                date_created = date_created.x,
                date_created_question_details = date_created.y
              )
            , by = c("user_assessment_session_id", "user_interaction_id", "assessment_item_id", "assessment_id"))
```

The skillIQ and roleIQ tests are addictive. I haven't used Pluralsight to learn and improve my technical skills yet, but I can see how the assessments would drive interaction and frequent improvement of subscribers. What a fun way to encourage personal and professional development!

# Data Exploration Questions

### 1. Describe and visualize how the distributions of user and question rankings compare and relate between assessments.

#### User Ranking Distributions

###### Overall Ranking Metrics

Using the `user_assessment_sessions` dataset we can see the distribution of the various metrics for the `r user_assessment_sessions %>% nrow()` user sessions.

```{r q1_overall, echo = F, warning = F, message = F, error = F}
# d %>% 
#   group_by(user_assessment_id, user_id, user_assessment_session_id) %>%
#   slice(1) %>%
#   ungroup() %>%
#   select(ends_with("overall"), n_questions_answered, -date_created_overall) %>%
## easier way may have been to just use user_assessment_sessions
user_assessment_sessions %>%
  select(ranking, rd, display_score, percentile) %>%
  gather() %>%
  ggplot(aes(x = value)) +
    geom_density() +
    facet_wrap(~ key, scales = "free") +
    theme_bw()
```

A comparison of the assessments is shown below. The consistency initially surprised me, but it makes sense that the distribution across the assessments varies mildly given the need to provide a standardized evaluation process agnostic to the actual assessment.

```{r q1_overall_byAssessment, echo = F, warning = F, message = F, error = F}
# d %>% 
#   group_by(user_assessment_id, user_id, user_assessment_session_id) %>%
#   slice(1) %>%
#   ungroup() %>%
#   select(ends_with("overall"), n_questions_answered, -date_created_overall) %>%
## easier way may have been to just use user_assessment_sessions
user_assessment_sessions %>%
  select(name, ranking, rd, display_score, percentile) %>%
  gather(,,-name) %>%
  ggplot(aes(x = value, color = name)) +
    geom_density() +
    facet_wrap(~ key, scales = "free") +
    labs(x = "", color = "Assessment") +
    theme_bw()

```


###### Interaction Ranking Metrics

A look at the distributions of the `user_interactions` dataset again shows some variability between assessments (which may be important if I knew more about the methodology), but no more than expected.

```{r q1_interactions, echo = F, warning = F, message = F, error = F}
user_interactions %>% 
  left_join(user_assessment_sessions %>%
              select(user_assessment_session_id, name), 
            by = "user_assessment_session_id") %>%
  select(name, ranking, rd, display_score, percentile) %>%
  gather(,, -name) %>%
  ggplot(aes(x = value, color = name)) +
    geom_density() +
    facet_wrap(~ key, scales = "free") +
    labs(x = "", color = "Assessment") +
    theme_bw()
```


#### Question Ranking Distributions

###### Question Ranking Metrics by Assessment

The distributions of `rd` and `client_elapsed_time` are heavily skewed so I took the log (base 10) of `client_elapsed_time` and dropped `rd` altogether. Later (question 5) we see that the question based rd value is generally 30 for the majority of the question interactions. I also removed the records with outliers in the `client_elapsed_time`; values > 99th percentile = 147260.3 and values < 0.

Per the plot we see that the distributions of `client_elapsed_time` for the assessments are very similar, though React is slightly shifted to the right of the other three assessments. That may be significant given it's presented on the log scale. The distributions of the `ranking` metric vary in a noisy way but still follow the same general structure.

```{r q1b_overall_byAssessment, echo = F, warning = F, message = F, error = F}
question_interactions %>%
  select(user_assessment_session_id, ranking, client_elapsed_time) %>%
  left_join(user_assessment_sessions %>% select(user_assessment_session_id, name),
            by = "user_assessment_session_id") %>%
  select(-user_assessment_session_id) %>%
  filter(
    client_elapsed_time < 1472560.3,
    client_elapsed_time > 0
  ) %>%
  mutate(client_elapsed_time_log = log10(client_elapsed_time)) %>%
  select(-client_elapsed_time) %>%
  gather(,,-name) %>%
  ggplot(aes(x = value, color = name)) +
    geom_density() +
    facet_wrap(~ key, scales = "free") +
    labs(x = "", color = "Assessment") +
    theme_bw()
```


### 2. How does it appear the algorithm determines when a user’s assessment session is complete?

We can evaluate the algorithm's determination to stop asking questions using a time-series of each assessment. The obvious guess is a minimal threshold for question-to-question changes in the RD value. Something very similar to that guess is confirmed by observing a random sample of several `user_assessment_session_id`s.

```{r q2_example_table, echo = F, warning = F, message = F, error = F}
# user_interactions %>%
#   filter(user_assessment_session_id == 1080045) %>% 
#   arrange(date_create)
```

```{r q2_example_plot, echo = F, warning = F, message = F, error = F}
q2data <- user_interactions %>%
  # filter(user_assessment_session_id == 1080045) %>% 
  group_by(user_assessment_session_id) %>%
  # filter(user_assessment_session_id %in% c(1122456, 1131712, 1143004))
  sample_n_groups(3) 

q2data %>%
  ggplot(aes(x = date_created, y = rd)) +
    geom_point() +
    geom_line() +
    theme_bw() +
    facet_wrap(~ user_assessment_session_id, scales = "free_x") + 
    labs(x = "Time of Question", y = "Ratings Reliability or Deviation (RD)")
```

It's probably worth checking the other metrics associated with a session (`display_score`, `percentile`, and `ranking`) to confirm our suspicions regarding `rd` as the main variable driving the algorithm. Per the plots below of the same three assessment sessions we see that `rd` is the only metric of the four that seems an appropriate option.

```{r q2_example_plot_multiple_metrics, echo = F, warning = F, message = F, error = F, fig.height = 6}
q2data %>%
  select(-user_interaction_id, -assessment_item_id) %>%
  gather(,,-user_assessment_session_id, -date_created) %>%
  ggplot(aes(x = date_created, y = value)) +
    geom_point() +
    geom_line() +
    theme_bw() +
    facet_grid(key ~ user_assessment_session_id, scales = "free") + 
    labs(x = "Time of Question", y = "")
```

A closer look at the distribution of the minimum `rd` values of each assessment's interaction shows that a simple threshold of 80 drives the stopping rule. Over 75% of the sessions were stopped at a `rd` value below and very near 80. While that seems like an arbitrary value to me, I am sure there was some empirical and theoretical studies performed to determine that threshold. Also 75% may seem low, but that includes all sessions, even those that were stopped prematurely by the user (as discussed in question 3).


```{r q2_quantiles, echo = F, warning = F, message = F, error = F, eval = F}
user_interactions %>% 
  group_by(user_assessment_session_id) %>% 
  summarise(minRD = min(rd, na.rm = T)) %>% 
  pull(minRD) %>% 
  quantile(probs = seq(0, 1, len = 21))
```


### 3. Which of the assessments has the highest and lowest dropout rates, respectively?

Assuming the threshold for completing the assessment is 80, then the overall dropout rate is around 25% (1608/6678 = 24.07%). The dropout rates for each assessment vary substantially, including React at 34.9%, Illustrator at 32.1%, Python at 21.8%, and Javascript at 21.0%.

```{r q3_rd_threshold_table, echo = F, warning = F, message = F, error = F, eval = F}
user_assessment_sessions %>% 
  mutate(rd_threshold = if_else(rd <= 80, "RD Threshold Met (RD <= 80)", "RD Threshold Unmet (RD > 80)")) %>% 
  group_by(rd_threshold) %>% 
  summarise(n())
```


```{r q3_rd_threshold_table2, echo = F, warning = F, message = F, error = F, eval = F}
user_assessment_sessions %>% 
  mutate(rd_threshold = if_else(rd <= 80, "RD Threshold Met (RD <= 80)", "RD Threshold Unmet (RD > 80)")) %>% 
  group_by(name, rd_threshold) %>% 
  summarise(n())
```


```{r q3_rd_distn_vs_numQs, echo = F, warning = F, message = F, error = F, fig.height = 4}
user_assessment_sessions %>% 
  mutate(rd_threshold = if_else(rd <= 80, "RD Threshold Met (RD <= 80)", "RD Threshold Unmet (RD > 80)")) %>% 
  ggplot(aes(x = n_questions_answered, color = name)) + 
    geom_density() + 
    facet_wrap( ~ rd_threshold, scales = "free_y") + 
    theme_bw() +
    labs(x = "Number of Questions Answered", color = "Assessment")
```

While the plot below doesn't quite tell the full story it does help illustrate that it generally takes 18-20 questions answered to obtain the `rd` threshold of 80. There is a subtle negative slope to the blue points indicating that the users who dropout early (< 10 questions) were likely incorrectly answering the majority of the few questions they actually answered. A closer look using an approach similar to how what I did in question 5 may allow us to test that hypothesis.


```{r q3_scatter_rdVSdisplayscore, echo = F, warning = F, message = F, error = F, fig.height = 4}
user_assessment_sessions %>% 
  ggplot(aes(x = rd, y = display_score, color = n_questions_answered)) + 
    geom_point(alpha = 0.25) + 
    theme_bw() + 
    facet_wrap(~ name) + 
    scale_color_gradientn(colors = matlab::jet.colors(100)) +
  labs(x = "RD", y = "Display Score", color = "Number of\nQuestions\nAnswered")
```

Javascript has some unusually high number of questions answered at the highest end of the `display_score` that still don't quite result in a `rd` value dropping below the threshold. That may be something worth examining in more detail.


### 4. Is there significant variance in question difficulty by topic within a given assessment?

To measure question difficulty I chose to calculate how often (percent) a user answered a given question correctly. The density plots below show the range of question difficulty for each Assessment:Topic combination. Clearly the range of question difficulty varies greatly across the Assessment:Topic combinations. Some topics, like *Python: Scalars and Operators* and *Illustrator: Transforming and Managing Objects*, span nearly the entire range of values showing both easy (questions frequently answered correctly) and difficult questions. Other topics, like *React: Forms* and *Javascript: Exceptions*, have questions that are generally answered correctly the same percent of the time. For such topics this lack of variability may make it hard to differentiate scores and rankings when compared with topics containing more variety in the difficulty of questions.

```{r q4_densityplot, echo = F, warning = F, message = F, error = F, fig.height = 7}
question_interactions %>% 
  left_join(question_details, 
            by = "item_content_id") %>% 
  left_join(user_assessment_sessions %>% select(user_assessment_session_id, assessment_id, name), 
            by = c("user_assessment_session_id", "assessment_id")) %>%
  group_by(name, topic_name, assessment_item_id, topic_name) %>%
  summarise(percent_correct = 100*sum(result == "Correct") / length(result)) %>%
  mutate(label = paste(name, topic_name, sep = ": ")) %>%
  ggplot(aes(x = percent_correct)) +
    geom_density() +
    theme_bw() +
    scale_x_continuous(limits = c(0,100)) +
    facet_wrap(~ label, scales = "free_y", ncol = 4) +
    labs(x = "Percent Question was Answered Correctly", y = "Number of User Assessments") +
    theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())
```

The range (max - min) of question difficulty (percent correctly answered) for each Assessment:Topic combination is listed below. I didn't check the frequency of the questions or of the topics which would affect both the percent correctly answered and possibly the range of difficulty.

```{r q4_table, echo = F, warning = F, message = F, error = F}
q4_table <- question_interactions %>% 
  left_join(question_details, 
            by = "item_content_id") %>% 
  left_join(user_assessment_sessions %>% select(user_assessment_session_id, assessment_id, name), 
            by = c("user_assessment_session_id", "assessment_id")) %>%
  group_by(name, topic_name, assessment_item_id, topic_name) %>%
  summarise(percent_correct = 100*sum(result == "Correct") / length(result)) %>%
  mutate(`Assessment:Topic` = paste(name, topic_name, sep = ": ")) %>% 
  group_by(`Assessment:Topic`, name, topic_name) %>% 
  summarise(range = round(max(percent_correct)-min(percent_correct), 1)) %>%
  ungroup() %>%
  select(topic_name, range)

kable(q4_table) %>%
  kable_styling(bootstrap_options = "striped", 
                full_width = F, 
                position = "left") %>%
  row_spec(0, font_size = 0) %>%
  pack_rows("Illustrator", 1, 9) %>%
  pack_rows("Javascript", 10, 18) %>%
  pack_rows("Unknown", 19, 19) %>%
  pack_rows("Python", 20, 28) %>%
  pack_rows("React", 29, 38) %>%
  scroll_box(width="100%", height="400px")
```


### 5. How many times must a question be answered before it reaches its certainty floor? Does that number appear to be constant or does it vary depending on question or assessment?

There are `r question_interactions %>% left_join(question_details, by = "item_content_id") %>% pull(assessment_item_id) %>% unique() %>% length()` questions in the dataset. I expect the `rd` metric to again indicate the certainty floor. A quick look at the distribution of `rd` values shows that floor to be 30. However, many (71.1%) of the `assessment_item_id`s show all of their `rd` values to equal 30. Maybe that's because those are older questions that reached the floor (30) previous to this dataset.

```{r q5_k, echo = F, warning = F, message = F, error = F, eval = F}
question_interactions %>% 
  left_join(question_details, by = "item_content_id") %>% 
  pull(assessment_item_id) %>% 
  unique() %>% 
  length()
```

```{r q5_density_all30vsNot, echo = F, warning = F, message = F, error = F, fig.height = 6, eval = F}
question_interactions %>% 
  left_join(question_details, by = "item_content_id") %>% 
  group_by(assessment_item_id) %>% 
  summarise(
    all30 = if_else(all(rd == 30), 1, 0), 
    count = n()
  ) %>% 
  ggplot(aes(x = count)) + 
    geom_density() + 
    theme_bw() + 
    facet_wrap(~all30, scales = "free")
```

We would really like to look at all `r question_interactions %>% left_join(question_details, by = "item_content_id") %>% pull(assessment_item_id) %>% unique() %>% length()` of these questions. We could examine much of the structure using trelliscopejs, a tool for interactively viewing a large collection of visualizations. The key opportunity when using trelliscope is that it allows for creation of a rich feature set that is then used to sort and filter through the data helping us see nuances, outliers, and important features of that data. 

A brief description of the cognostics (features) is available by clicking on the "i" in the upper left corner. You can search for interesting `assessment_item_id`s by using the **Sort** and **Filter** buttons on the left hand side. To see those `assessment_item_id`s that have values of `rd` other than 30, click on the Filter button, then on the "All RD values = 30" pill, then enter "0" into the right side. This will reduce the total number of panels from 724 to 209. To see panels (plots) where at least two points are present (and thus a plot is created), remain clicked into the Filter button, then click on the "Number of Question Interactions" pill, then enter 2 on the left hand size of the range selection. This immediately removes all the blank panels (not plotted because only one observation exists) and reduces the number of panels from 209 to 180. Clicking on the Filter button again closes that window. You can sort or filter further to test hypotheses or explore the data sliced by `assessment_item_id`. Happy exploring!

Also note that the plotting panel function can be ggplot or rbokeh based. Here I used bokeh so even within the plot some interactivity exists.

```{r q5_trelliscope1, echo = F, warning = F, message = F, error = F, fig.height = 6}
rd_not_30_assessment_ids <- question_interactions %>% 
  group_by(assessment_item_id) %>%
  summarise(
    all30 = if_else(all(rd == 30), 1, 0),
    count = n()
  )


question_interactions %>% 
  left_join(question_details, by = "item_content_id") %>% 
  rename(date_created = date_created.x) %>%
  # add back in the summary info though not really necessary
  left_join(rd_not_30_assessment_ids, by = "assessment_item_id") %>%
  group_by(assessment_item_id) %>%
  # nest as a tibble to setup for trelliscope
  nest() %>%
  # create feature set (cogs) and panel plot function to apply to each assessment_item_id
  mutate(
        cogs = map_cog(data, ~ data_frame(
          `Max RD` = cog(max(.$rd), desc = "Max RD"),
          `All RD values = 30` = .$all30[1],
          `Number of Question Interactions` = .$count[1],
          `Min Date` = cog(min(.$date_created), desc = "Min date_created"),
          `Max Date` = cog(max(.$date_created), desc = "Max date_created"),
          `Avg Client Elapsed Time` = cog(mean(.$client_elapsed_time), desc = "Average Client Elapsed Time"),
          `Std Dev Client Elapsed Time` = cog(sd(.$client_elapsed_time), desc = "Standard Deviation of Client Elapsed Time"),
          `Avg Ranking` = cog(mean(.$ranking), desc = "Average Ranking"),
          `Pct Correct` = cog(round(100*sum(.$result == "Correct") / length(.$result), 1), desc = "Percent of Results Correct")
        )),
    panel = map_plot(data, ~ figure(xlab = "", ylab = "RD") %>% 
                         ly_points(x = date_created, 
                                   y = rd, 
                                   alpha = .5, 
                                   size = 8, 
                                   color = result, 
                                   hover = data_frame(
                                     `Date Created` = .x$date_created, 
                                     Ranking = .x$ranking,
                                     RD = .x$rd, 
                                     Result = .x$result,
                                     `Client Elapsed Time` = .x$client_elapsed_time,
                                     `Topic Name` = .x$topic_name), 
                                   data = .x) %>%
                         ly_abline(h = 30, legend = F)
    )) %>%
    trelliscope(name = "question_rd", 
                group = "explore", 
                path = "~/Documents/Development/R/trelliscope", 
                self_contained = TRUE)
```

Obviously a `rd` value of 30 is important and relevant, but I didn't find anything else that gave me sufficient confidence to answer this question explicitly beyond saying there appears to be plenty of variation between questions. 

Maybe I should have used a similar approach (trelliscope) to examen the various assessments in the previous questions.

---

# More Involved/Open-ended Questions

### 1. Identify a metric that could be used to identify questions that are performing poorly, and consequently might need to be reviewed, changed, or removed.

- Questions that render a nearly always incorrect answer, especially when the question difficulty is comparatively low. (Some questions are likely purposefully difficult so one expects those to rarely have a correct response.)
- Questions that increase the RD metric substantially (though that may be a function of question order).
- (thinking of a scatterplot comparing rd change due to that question vs current percentile of the user, meaning some identification of outliers occurring when rd change is high and negative and percentile was low)

### 2. Suppose an update to Python causes a question’s answer to change, but our question authors don’t notice, and the now-outdated question remains in the test. How might that scenario reveal itself in the data?

Hopefully it reveals itself as often rendering an incorrect response. That may not be true of more experienced or long-time users of that technology/language so one might need to account for that somehow. I noticed a link at the bottom of the page after the answer is revealed that provided an opportunity for a situation like this to be identified.

### 3. Given your response to number 2 in the Data Exploration Questions above, what is a method we could use to determine ideal points to stop a user’s assessment session (i.e. identify the right balance between certainty and burden on the user)?

You could compare the `display_score` (and ranking) to the changes/decreases in `rd` as you approach the rd threshold. Some rules or a model may be applied to allow you to occasionally shave off 3-5 questions per assessment if you sufficiently understood the relationship between `display_score` and `rd` at the late stage of the assessment.

I suppose you could try to account for the distribution/curve of previous assessments of that user. For example, if they have taken several assessments before the current assessment you may be able to predict/extrapolate the end score and ranking based on their position part way through the assessment.

Taking that a wild step further, why not treat each step of an assessment (for a giving topic) as a modeling and prediction opportunity by developing a deep learning model trained to the eventual outcome of the assessment. That way you could use the thousands (or millions) of assessments for that topic to generate a prediction such that you could stop the assessment once the prediction has reached a certain threshold of accuracy per the model. Just to be clear I am thinking of a different deep learning model (or potentially any predictive model) for each set of questions of a given topic in order. That wasn't very clear so ... one model based on five questions answered, then a model based on six questions answered, and so on.

### 4. How could we calculate the overall difficulty level of a particular topic? How might we then calculate a topic-level score for a single user?

You may get close by determining what combination of topics tend to be taken by users. If a set of users are prone to take the same five topic assessments (and rarely other topics) then you could look to see if which topic was the most difficult to that group. As an example, business analysts may consistently take the data warehousing, data analytics/visualization, SQL, and Python assessments and often struggle score lower in the Python assessment. 

I wonder if the frequency of the topic assessed is an indicator of the difficulty. Certainly the frequency relates to the popularity and the general demand/usefulness of the topic; as well as the newness of the topic (newer tools/tech/languages may be taken less frequently - following an adoption curve). Fortran or other older languages/technologies may be considered more difficult simply because less modern learning methods exist for them.

How is "difficult" defined here?


